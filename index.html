<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>ACQUIRE 2026 — The 1st International Workshop on AI Code QUality, Integrity &amp; REliability</title>
<link rel="stylesheet" href="/2026/css/style.css" />
<!-- Google Fonts for samples (Inter, Merriweather, Roboto Mono) -->
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&amp;family=Merriweather:wght@400;700&amp;family=Roboto+Mono:wght@400;700&amp;display=swap" rel="stylesheet">
<meta name="description" content="ACQUIRE at EDCC 2026: dependability and quality of AI-generated code and code assistants." />
</head>
<body>
  <!-- header -->
  <header>
    <nav class="nav">
      <a href="/2026/index.html" aria-current="page">Home</a>
      <a href="/2026/html/papers.html">Call for Papers</a>
      <a href="/2026/html/participation.html">Call for Participation</a>
      <a href="/2026/html/program.html">Program</a>
      <!-- <a href="/2026/html/keynote.html">Keynote</a> -->
      <a href="/2026/html/committee.html">Committee</a>
    </nav>
  </header>

  <!-- HERO (new) -->
  <section class="hero">
    <div class="hero-inner">
      <div class="container center">
        <h1>ACQUIRE 2026</h1>
        <h2>The 1st International Workshop on AI Code QUality, Integrity &amp; REliability</h2>
        <!-- <h2>Co-located with <a href="https://www.cs.kent.ac.uk/EDCC2026/home">EDCC 2026</a> — Canterbury, UK • 7-10 April 2026</h2> -->
         <h2> Tue 7 April 2026, Canterbury, UK</h2>
      </div>
  </section>

  <main class="container">
    <h1>About ACQUIRE</h1>
    <h1></h1>
    <h1></h1>

    <p>Large Language Models are already writing, reviewing, and repairing code, yet the community lacks a rigorous, shared basis for judging whether these AI-produced artifacts are dependable throughout real software lifecycles. Today’s emphasis on benchmark accuracy obscures risks that matter in practice: silent hallucinations, insecure toolchains and prompts, brittle behavior under distribution shift, opaque provenance, and evidence that cannot be audited or reproduced.
ACQUIRE’26 responds to this gap by convening AI and Software Engineering researchers and practitioners to refocus the conversation from raw performance to verifiable quality, grounded in auditable taxonomies and metrics, assurance cases and transparent and reproducible evidence. The workshop’s aim is to make AI-for-code not just powerful, but trustworthy and dependable, encompassing all aspects of software quality, including security, maintainability, correctness and performance.</p>

    <p>
We welcome contributions that address the following areas: 
<ul>
<li><em>Quality &amp; assurance</em>: taxonomies, auditable metrics, conformance profiles, and safety/assurance cases for LLMs and agents for code tasks;</li>
<li><em>Security of models &amp; supply chain</em>: threat models spanning models, data, prompts, and toolchains, with provenance, SBOM/AI-BOM, signing, and attestation;</li>
<li><em>Robustness in practice</em>: hallucination detection/mitigation, shift- and fault-tolerance, vulnerability detection and patch quality, and CI/CD gating with runtime guards;</li>
<li><em>Evidence &amp; reproducibility</em>: open benchmarks, standardized reporting for datasets, metrics, prompts, agents, and protocols, and certification-oriented evaluation.</li>
</ul>


    <p>Cross-cutting themes include human-AI collaboration (uncertainty display, attribution), and the impact of AI on maintainability and technical debt. We welcome empirical studies, methods, tools, and experience reports, especially those that deliver auditable evidence, align on taxonomies and reporting schemas, advance provenance and attestation practices, and demonstrate robust, reproducible evaluation under real-world and adversarial conditions.</p>

    <p>Submissions reporting negative results or unexpected findings are also welcome, as they offer valuable insights.</p>

    <h2>ACQUIRE 2026 is co-located with <a href="https://www.cs.kent.ac.uk/EDCC2026/home">EDCC 2026</a> — Canterbury, UK • 7-10 April 2026</h2>
    <!-- <div class="callout">
      <p><strong>Publication:</strong> Accepted papers will appear in the IEEE Computer Society supplemental proceedings (archival &amp; indexed).</p>
      <p><strong>Format:</strong> Preferred format is a half-day event.</p>
    </div> -->

    <div class="container">
        <p>
          <a class="btn btn-primary" href="/2026/html/papers.html">Submit a Paper</a>
          <a class="btn btn-ghost" href="/2026/html/participation.html">Registration Info</a>
          <p class="small">Deadlines in AoE <kbd>Anywhere on Earth</kbd></p>
        </p>
    </div>

    <h1>Topics of Interest</h1>

    <p>This call for papers invites all researchers and practitioners to explore the quality and reliability aspects of AI in the SE field. The workshop will cover a wide range of topics, including but not limited to:</p>

        <div class="shift-right">
  <h3>Foundations of Quality &amp; Dependability for Code LLMs</h3>
  <ul>
    <li>Taxonomies &amp; reference models</li>
    <li>Measurement frameworks &amp; methodology</li>
    <li>Assurance/safety cases for LLMs &amp; agents</li>
    <li>Open benchmarks &amp; leaderboards; standardized reporting (datasets, metrics, protocols)</li>
  </ul>
</div>

<div class="shift-right">
  <h3>Software Quality for AI-Generated Code</h3>
  <ul>
    <li>Correctness &amp; robustness; hallucination detection &amp; mitigation</li>
    <li>Maintainability &amp; technical-debt control</li>
    <li>Portability &amp; compatibility</li>
    <li>Efficiency &amp; performance</li>
  </ul>
</div>

<div class="shift-right">
  <h3>Security of the LLM Supply Chain</h3>
  <ul>
    <li>Threat models (model/data/prompt/toolchain)</li>
    <li>Signing &amp; attestation; SBOMs/AI-BOMs</li>
    <li>Dependency risk in plug-ins/RAG/agents</li>
    <li>Governance, audits &amp; compliance</li>
  </ul>
</div>

<div class="shift-right">
  <h3>Attacks &amp; Mitigations on LLMs</h3>
  <ul>
    <li>Data poisoning</li>
    <li>Adversarial prompts &amp; jailbreaks</li>
    <li>Prompt-injection &amp; tool-use exploits</li>
    <li>Guardrails, policy-as-code, content filters, red teaming</li>
    <li>Attack/defense benchmarks</li>
  </ul>
</div>

<!-- <div class="shift-right">
  <h3>Bias &amp; Fairness in SE Tasks</h3>
  <ul>
    <li>Detection &amp; mitigation</li>
    <li>Fairness metrics &amp; trade-offs</li>
    <li>Impact on quality, security &amp; developer experience</li>
  </ul>
</div> -->

<div class="shift-right">
  <h3>Human-AI Collaboration</h3>
  <ul>
    <li>Comparative studies (correctness, speed, maintainability, security)</li>
    <li>Pair-programming &amp; review workflows</li>
    <li>Explainability, attribution &amp; uncertainty; bias &amp; fairness</li>
    <li>Cognitive load &amp; UX</li>
  </ul>
</div>

<div class="shift-right">
  <h3>Privacy, Licensing, Provenance &amp; Integrity</h3>
  <ul>
    <li>Privacy-preserving methods; leakage/memorization controls</li>
    <li>Licensing compliance</li>
    <li>Provenance &amp; traceability at scale</li>
    <li>Legal, ethical &amp; regulatory aspects</li>
  </ul>
</div>
<!-- 
<div class="shift-right">
  <h3>Quality &amp; Security Hardening in the SDLC</h3>
  <ul>
    <li>Static/dynamic analysis, fuzzing, property-based testing</li>
    <li>Self-repair &amp; self-verification</li>
    <li>CI/CD gates, risk scoring &amp; runtime guards</li>
  </ul>
</div> -->

<div class="shift-right">
  <h3>Green AI for Software Engineering</h3>
  <ul>
    <li>Energy/compute profiling</li>
    <li>Efficiency-aware prompting &amp; inference</li>
    <li>Carbon-aware deployment</li>
    <li>Quality-cost-environment trade-offs</li>
  </ul>
</div>

<div class="shift-right">
  <h3>Vulnerability Detection &amp; Patching</h3>
  <ul>
    <li>Secure-by-construction patterns</li>
    <li>Ground-truth benchmarks</li>
    <li>Patch quality, regressions &amp; assurance</li>
  </ul>
</div>

  <!-- footer -->
  <footer>
    <div class="container small">© 2026 ACQUIRE Workshop</div>
  </footer>
</body>
</html>
